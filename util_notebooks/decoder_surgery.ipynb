{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7947014",
   "metadata": {},
   "source": [
    "## VAE Decoder Surgery/Inflation to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e4d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use sdp attention as default\n",
      "keep default attention mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argar/miniconda3/envs/hypir/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from typing import Optional, Any\n",
    "from omegaconf import OmegaConf, DictConfig, ListConfig\n",
    "from copy import copy\n",
    "import importlib\n",
    "import re\n",
    "from torchsummary import summary\n",
    "\n",
    "from gqvr.model.distributions import DiagonalGaussianDistribution\n",
    "from gqvr.model.config import Config, AttnMode\n",
    "from gqvr.model.matt_2Drnn_stabilizer import ControlledConvEMAStabilizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9af628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinearity(x):\n",
    "    # swish\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def Normalize(in_channels, num_groups=32):\n",
    "    return torch.nn.GroupNorm(\n",
    "        num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True\n",
    "    )\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            self.conv = torch.nn.Conv2d(\n",
    "                in_channels, in_channels, kernel_size=3, stride=1, padding=1\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
    "        if self.with_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            # no asymmetric padding in torch conv, must do it ourselves\n",
    "            self.conv = torch.nn.Conv2d(\n",
    "                in_channels, in_channels, kernel_size=3, stride=2, padding=0\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.with_conv:\n",
    "            pad = (0, 1, 0, 1)\n",
    "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
    "            x = self.conv(x)\n",
    "        else:\n",
    "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels,\n",
    "        out_channels=None,\n",
    "        conv_shortcut=False,\n",
    "        dropout,\n",
    "        temb_channels=512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "\n",
    "        self.norm1 = Normalize(in_channels)\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        if temb_channels > 0:\n",
    "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
    "        self.norm2 = Normalize(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                self.conv_shortcut = torch.nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "                )\n",
    "            else:\n",
    "                self.nin_shortcut = torch.nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=1, padding=0\n",
    "                )\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = x\n",
    "        h = self.norm1(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        if temb is not None:\n",
    "            h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None]\n",
    "\n",
    "        h = self.norm2(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                x = self.conv_shortcut(x)\n",
    "            else:\n",
    "                x = self.nin_shortcut(x)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        print(f\"building AttnBlock (vanilla) with {in_channels} in_channels\")\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.k = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.v = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.proj_out = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b, c, h, w = q.shape\n",
    "        q = q.reshape(b, c, h * w)\n",
    "        q = q.permute(0, 2, 1)  # b,hw,c\n",
    "        k = k.reshape(b, c, h * w)  # b,c,hw\n",
    "        w_ = torch.bmm(q, k)  # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        w_ = w_ * (int(c) ** (-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = v.reshape(b, c, h * w)\n",
    "        w_ = w_.permute(0, 2, 1)  # b,hw,hw (first hw of k, second of q)\n",
    "        h_ = torch.bmm(v, w_)  # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = h_.reshape(b, c, h, w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x + h_\n",
    "\n",
    "\n",
    "class MemoryEfficientAttnBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses xformers efficient implementation,\n",
    "    see https://github.com/MatthieuTPHR/diffusers/blob/d80b531ff8060ec1ea982b65a1b8df70f73aa67c/src/diffusers/models/attention.py#L223\n",
    "    Note: this is a single-head self-attention operation\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        print(\n",
    "            f\"building MemoryEfficientAttnBlock (xformers) with {in_channels} in_channels\"\n",
    "        )\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.k = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.v = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.proj_out = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.attention_op: Optional[Any] = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        B, C, H, W = q.shape\n",
    "        q, k, v = map(lambda x: rearrange(x, \"b c h w -> b (h w) c\"), (q, k, v))\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: t.unsqueeze(3)\n",
    "            .reshape(B, t.shape[1], 1, C)\n",
    "            .permute(0, 2, 1, 3)\n",
    "            .reshape(B * 1, t.shape[1], C)\n",
    "            .contiguous(),\n",
    "            (q, k, v),\n",
    "        )\n",
    "        out = Config.xformers.ops.memory_efficient_attention(\n",
    "            q, k, v, attn_bias=None, op=self.attention_op\n",
    "        )\n",
    "\n",
    "        out = (\n",
    "            out.unsqueeze(0)\n",
    "            .reshape(B, 1, out.shape[1], C)\n",
    "            .permute(0, 2, 1, 3)\n",
    "            .reshape(B, out.shape[1], C)\n",
    "        )\n",
    "        out = rearrange(out, \"b (h w) c -> b c h w\", b=B, h=H, w=W, c=C)\n",
    "        out = self.proj_out(out)\n",
    "        return x + out\n",
    "\n",
    "\n",
    "class SDPAttnBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        print(f\"building SDPAttnBlock (sdp) with {in_channels} in_channels\")\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.k = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.v = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.proj_out = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        B, C, H, W = q.shape\n",
    "        q, k, v = map(lambda x: rearrange(x, \"b c h w -> b (h w) c\"), (q, k, v))\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: t.unsqueeze(3)\n",
    "            .reshape(B, t.shape[1], 1, C)\n",
    "            .permute(0, 2, 1, 3)\n",
    "            .reshape(B * 1, t.shape[1], C)\n",
    "            .contiguous(),\n",
    "            (q, k, v),\n",
    "        )\n",
    "        out = F.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "        out = (\n",
    "            out.unsqueeze(0)\n",
    "            .reshape(B, 1, out.shape[1], C)\n",
    "            .permute(0, 2, 1, 3)\n",
    "            .reshape(B, out.shape[1], C)\n",
    "        )\n",
    "        out = rearrange(out, \"b (h w) c -> b c h w\", b=B, h=H, w=W, c=C)\n",
    "        out = self.proj_out(out)\n",
    "        return x + out\n",
    "\n",
    "\n",
    "def make_attn(in_channels, attn_type=\"vanilla\", attn_kwargs=None):\n",
    "    assert attn_type in [\n",
    "        \"vanilla\",\n",
    "        \"sdp\",\n",
    "        \"xformers\",\n",
    "        \"linear\",\n",
    "        \"none\",\n",
    "    ], f\"attn_type {attn_type} unknown\"\n",
    "    if attn_type == \"vanilla\":\n",
    "        assert attn_kwargs is None\n",
    "        return AttnBlock(in_channels)\n",
    "    elif attn_type == \"sdp\":\n",
    "        return SDPAttnBlock(in_channels)\n",
    "    elif attn_type == \"xformers\":\n",
    "        return MemoryEfficientAttnBlock(in_channels)\n",
    "    elif attn_type == \"none\":\n",
    "        return nn.Identity(in_channels)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        ch,\n",
    "        out_ch,\n",
    "        ch_mult=(1, 2, 4, 8),\n",
    "        num_res_blocks,\n",
    "        attn_resolutions,\n",
    "        dropout=0.0,\n",
    "        resamp_with_conv=True,\n",
    "        in_channels,\n",
    "        resolution,\n",
    "        z_channels,\n",
    "        double_z=True,\n",
    "        use_linear_attn=False,\n",
    "        **ignore_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ### setup attention type\n",
    "        if Config.attn_mode == AttnMode.SDP:\n",
    "            attn_type = \"sdp\"\n",
    "        elif Config.attn_mode == AttnMode.XFORMERS:\n",
    "            attn_type = \"xformers\"\n",
    "        else:\n",
    "            attn_type = \"vanilla\"\n",
    "        if use_linear_attn:\n",
    "            attn_type = \"linear\"\n",
    "        self.ch = ch\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # downsampling\n",
    "        self.conv_in = torch.nn.Conv2d(\n",
    "            in_channels, self.ch, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        curr_res = resolution\n",
    "        in_ch_mult = (1,) + tuple(ch_mult)\n",
    "        self.in_ch_mult = in_ch_mult\n",
    "        self.down = nn.ModuleList()\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_in = ch * in_ch_mult[i_level]\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                block.append(\n",
    "                    ResnetBlock(\n",
    "                        in_channels=block_in,\n",
    "                        out_channels=block_out,\n",
    "                        temb_channels=self.temb_ch,\n",
    "                        dropout=dropout,\n",
    "                    )\n",
    "                )\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            down.attn = attn\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res // 2\n",
    "            self.down.append(down)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(\n",
    "            in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(\n",
    "            in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(\n",
    "            block_in,\n",
    "            2 * z_channels if double_z else z_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # timestep embedding\n",
    "        temb = None\n",
    "\n",
    "        # downsampling\n",
    "        hs = [self.conv_in(x)]\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
    "                if len(self.down[i_level].attn) > 0:\n",
    "                    h = self.down[i_level].attn[i_block](h)\n",
    "                hs.append(h)\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
    "\n",
    "        # middle\n",
    "        h = hs[-1]\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # end\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        ch,\n",
    "        out_ch,\n",
    "        ch_mult=(1, 2, 4, 8),\n",
    "        num_res_blocks,\n",
    "        attn_resolutions,\n",
    "        dropout=0.0,\n",
    "        resamp_with_conv=True,\n",
    "        in_channels,\n",
    "        resolution,\n",
    "        z_channels,\n",
    "        give_pre_end=False,\n",
    "        tanh_out=False,\n",
    "        use_linear_attn=False,\n",
    "        **ignorekwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ### setup attention type\n",
    "        if Config.attn_mode == AttnMode.SDP:\n",
    "            attn_type = \"sdp\"\n",
    "        elif Config.attn_mode == AttnMode.XFORMERS:\n",
    "            attn_type = \"xformers\"\n",
    "        else:\n",
    "            attn_type = \"vanilla\"\n",
    "        if use_linear_attn:\n",
    "            attn_type = \"linear\"\n",
    "        self.ch = ch\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.give_pre_end = give_pre_end\n",
    "        self.tanh_out = tanh_out\n",
    "\n",
    "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
    "        in_ch_mult = (1,) + tuple(ch_mult)\n",
    "        block_in = ch * ch_mult[self.num_resolutions - 1]\n",
    "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
    "        self.z_shape = (1, z_channels, curr_res, curr_res)\n",
    "\n",
    "        # z to block_in\n",
    "        self.conv_in = torch.nn.Conv2d(\n",
    "            z_channels, block_in, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(\n",
    "            in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(\n",
    "            in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                block.append(\n",
    "                    ResnetBlock(\n",
    "                        in_channels=block_in,\n",
    "                        out_channels=block_out,\n",
    "                        temb_channels=self.temb_ch,\n",
    "                        dropout=dropout,\n",
    "                    )\n",
    "                )\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res * 2\n",
    "            self.up.insert(0, up)  # prepend to get consistent order\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(\n",
    "            block_in, out_ch, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # assert z.shape[1:] == self.z_shape[1:]\n",
    "        self.last_z_shape = z.shape\n",
    "\n",
    "        # timestep embedding\n",
    "        temb = None\n",
    "\n",
    "        # z to block_in\n",
    "        h = self.conv_in(z)\n",
    "\n",
    "        # middle\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # upsampling\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                h = self.up[i_level].block[i_block](h, temb)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        # end\n",
    "        if self.give_pre_end:\n",
    "            return h\n",
    "\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        if self.tanh_out:\n",
    "            h = torch.tanh(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class AutoencoderKL(nn.Module):\n",
    "\n",
    "    def __init__(self, ddconfig, embed_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(**ddconfig)\n",
    "        self.decoder = Decoder(**ddconfig)\n",
    "        assert ddconfig[\"double_z\"]\n",
    "        self.quant_conv = torch.nn.Conv2d(2 * ddconfig[\"z_channels\"], 2 * embed_dim, 1)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        moments = self.quant_conv(h)\n",
    "        posterior = DiagonalGaussianDistribution(moments)\n",
    "        return posterior\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.post_quant_conv(z)\n",
    "        dec = self.decoder(z)\n",
    "        return dec\n",
    "\n",
    "    def forward(self, input, sample_posterior=True):\n",
    "        posterior = self.encode(input)\n",
    "        if sample_posterior:\n",
    "            z = posterior.sample()\n",
    "        else:\n",
    "            z = posterior.mode()\n",
    "        dec = self.decode(z)\n",
    "        return dec, posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8092199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building SDPAttnBlock (sdp) with 512 in_channels\n",
      "building SDPAttnBlock (sdp) with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load(\"/home/argar/apgi/gQVR/configs/train/train_sd2gan_video.yaml\")\n",
    "vae = AutoencoderKL(cfg.model.vae_cfg.ddconfig, cfg.model.vae_cfg.embed_dim)\n",
    "decoder = vae.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea3cd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 512, 64, 64]         18,944\n",
      "├─Module: 1                              []                        --\n",
      "|    └─ResnetBlock: 2-1                  [-1, 512, 64, 64]         --\n",
      "|    |    └─GroupNorm: 3-1               [-1, 512, 64, 64]         1,024\n",
      "|    |    └─Conv2d: 3-2                  [-1, 512, 64, 64]         2,359,808\n",
      "|    |    └─GroupNorm: 3-3               [-1, 512, 64, 64]         1,024\n",
      "|    |    └─Dropout: 3-4                 [-1, 512, 64, 64]         --\n",
      "|    |    └─Conv2d: 3-5                  [-1, 512, 64, 64]         2,359,808\n",
      "|    └─SDPAttnBlock: 2-2                 [-1, 512, 64, 64]         --\n",
      "|    |    └─GroupNorm: 3-6               [-1, 512, 64, 64]         1,024\n",
      "|    |    └─Conv2d: 3-7                  [-1, 512, 64, 64]         262,656\n",
      "|    |    └─Conv2d: 3-8                  [-1, 512, 64, 64]         262,656\n",
      "|    |    └─Conv2d: 3-9                  [-1, 512, 64, 64]         262,656\n",
      "|    |    └─Conv2d: 3-10                 [-1, 512, 64, 64]         262,656\n",
      "|    └─ResnetBlock: 2-3                  [-1, 512, 64, 64]         --\n",
      "|    |    └─GroupNorm: 3-11              [-1, 512, 64, 64]         1,024\n",
      "|    |    └─Conv2d: 3-12                 [-1, 512, 64, 64]         2,359,808\n",
      "|    |    └─GroupNorm: 3-13              [-1, 512, 64, 64]         1,024\n",
      "|    |    └─Dropout: 3-14                [-1, 512, 64, 64]         --\n",
      "|    |    └─Conv2d: 3-15                 [-1, 512, 64, 64]         2,359,808\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Module: 2                         []                        --\n",
      "|    |    └─Upsample: 3-16               [-1, 512, 128, 128]       2,359,808\n",
      "|    └─Module: 2                         []                        --\n",
      "|    |    └─Upsample: 3-17               [-1, 512, 256, 256]       2,359,808\n",
      "|    └─Module: 2                         []                        --\n",
      "|    |    └─Upsample: 3-18               [-1, 256, 512, 512]       590,080\n",
      "├─GroupNorm: 1-2                         [-1, 128, 512, 512]       256\n",
      "├─Conv2d: 1-3                            [-1, 3, 512, 512]         3,459\n",
      "==========================================================================================\n",
      "Total params: 15,827,331\n",
      "Trainable params: 15,827,331\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 391.87\n",
      "==========================================================================================\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 1318.00\n",
      "Params size (MB): 60.38\n",
      "Estimated Total Size (MB): 1378.44\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 512, 64, 64]         18,944\n",
       "├─Module: 1                              []                        --\n",
       "|    └─ResnetBlock: 2-1                  [-1, 512, 64, 64]         --\n",
       "|    |    └─GroupNorm: 3-1               [-1, 512, 64, 64]         1,024\n",
       "|    |    └─Conv2d: 3-2                  [-1, 512, 64, 64]         2,359,808\n",
       "|    |    └─GroupNorm: 3-3               [-1, 512, 64, 64]         1,024\n",
       "|    |    └─Dropout: 3-4                 [-1, 512, 64, 64]         --\n",
       "|    |    └─Conv2d: 3-5                  [-1, 512, 64, 64]         2,359,808\n",
       "|    └─SDPAttnBlock: 2-2                 [-1, 512, 64, 64]         --\n",
       "|    |    └─GroupNorm: 3-6               [-1, 512, 64, 64]         1,024\n",
       "|    |    └─Conv2d: 3-7                  [-1, 512, 64, 64]         262,656\n",
       "|    |    └─Conv2d: 3-8                  [-1, 512, 64, 64]         262,656\n",
       "|    |    └─Conv2d: 3-9                  [-1, 512, 64, 64]         262,656\n",
       "|    |    └─Conv2d: 3-10                 [-1, 512, 64, 64]         262,656\n",
       "|    └─ResnetBlock: 2-3                  [-1, 512, 64, 64]         --\n",
       "|    |    └─GroupNorm: 3-11              [-1, 512, 64, 64]         1,024\n",
       "|    |    └─Conv2d: 3-12                 [-1, 512, 64, 64]         2,359,808\n",
       "|    |    └─GroupNorm: 3-13              [-1, 512, 64, 64]         1,024\n",
       "|    |    └─Dropout: 3-14                [-1, 512, 64, 64]         --\n",
       "|    |    └─Conv2d: 3-15                 [-1, 512, 64, 64]         2,359,808\n",
       "├─ModuleList: 1                          []                        --\n",
       "|    └─Module: 2                         []                        --\n",
       "|    |    └─Upsample: 3-16               [-1, 512, 128, 128]       2,359,808\n",
       "|    └─Module: 2                         []                        --\n",
       "|    |    └─Upsample: 3-17               [-1, 512, 256, 256]       2,359,808\n",
       "|    └─Module: 2                         []                        --\n",
       "|    |    └─Upsample: 3-18               [-1, 256, 512, 512]       590,080\n",
       "├─GroupNorm: 1-2                         [-1, 128, 512, 512]       256\n",
       "├─Conv2d: 1-3                            [-1, 3, 512, 512]         3,459\n",
       "==========================================================================================\n",
       "Total params: 15,827,331\n",
       "Trainable params: 15,827,331\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 391.87\n",
       "==========================================================================================\n",
       "Input size (MB): 0.06\n",
       "Forward/backward pass size (MB): 1318.00\n",
       "Params size (MB): 60.38\n",
       "Estimated Total Size (MB): 1378.44\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vae.decoder, (4,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEMA_Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        ch,\n",
    "        out_ch,\n",
    "        ch_mult=(1, 2, 4, 8),\n",
    "        num_res_blocks,\n",
    "        attn_resolutions,\n",
    "        dropout=0.0,\n",
    "        resamp_with_conv=True,\n",
    "        in_channels,\n",
    "        resolution,\n",
    "        z_channels,\n",
    "        give_pre_end=False,\n",
    "        tanh_out=False,\n",
    "        use_linear_attn=False,\n",
    "        ema_hidden_channels=64,\n",
    "        ema_hidden_layers=2,\n",
    "        ema_conv_kernel=3,\n",
    "        ema_fusion_kernel=1,\n",
    "        ema_skip_connection=True,\n",
    "        detach_memory=True,\n",
    "        **ignorekwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Attention type\n",
    "        if Config.attn_mode == AttnMode.SDP:\n",
    "            attn_type = \"sdp\"\n",
    "        elif Config.attn_mode == AttnMode.XFORMERS:\n",
    "            attn_type = \"xformers\"\n",
    "        else:\n",
    "            attn_type = \"vanilla\"\n",
    "        if use_linear_attn:\n",
    "            attn_type = \"linear\"\n",
    "\n",
    "        self.ch = ch\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.give_pre_end = give_pre_end\n",
    "        self.tanh_out = tanh_out\n",
    "        self.detach_memory = detach_memory\n",
    "\n",
    "        in_ch_mult = (1,) + tuple(ch_mult)\n",
    "        block_in = ch * ch_mult[self.num_resolutions - 1]\n",
    "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
    "        self.z_shape = (1, z_channels, curr_res, curr_res)\n",
    "\n",
    "        # z → block_in\n",
    "        self.conv_in = torch.nn.Conv2d(z_channels, block_in, 3, 1, 1)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout,)\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout,)\n",
    "\n",
    "        # --- EMA memories ---\n",
    "        self.ema_mem_block1_stab = None\n",
    "        self.ema_mem_block1_unstab = None\n",
    "        self.ema_mem_block2_stab = None\n",
    "        self.ema_mem_block2_unstab = None\n",
    "        self.ema_mem_convout_stab = None\n",
    "        self.ema_mem_convout_unstab = None\n",
    "\n",
    "        # --- EMA parameters per layer ---\n",
    "        def make_ema_params():\n",
    "            weights = nn.ParameterList()\n",
    "            biases = nn.ParameterList()\n",
    "            activations = nn.ModuleList([nn.LeakyReLU() for _ in range(ema_hidden_layers + 1)])\n",
    "            # Initialize parameters immediately\n",
    "            for i in range(ema_hidden_layers + 2):\n",
    "                w_shape = (ema_hidden_channels, ema_hidden_channels, ema_conv_kernel, ema_conv_kernel) \\\n",
    "                          if i < ema_hidden_layers else (ch, ema_hidden_channels, ema_conv_kernel, ema_conv_kernel)\n",
    "                b_shape = (ema_hidden_channels,) if i < ema_hidden_layers else (ch,)\n",
    "                weights.append(nn.Parameter(torch.empty(w_shape)))\n",
    "                biases.append(nn.Parameter(torch.zeros(b_shape)))\n",
    "                # He / Kaiming init for convs\n",
    "                if i < ema_hidden_layers:\n",
    "                    nn.init.kaiming_uniform_(weights[-1], a=0.2)\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(weights[-1], gain=1.0)\n",
    "                    nn.init.constant_(biases[-1], -4.0)\n",
    "            return weights, biases, activations\n",
    "\n",
    "        self.ema1_weights, self.ema1_biases, self.ema1_activations = make_ema_params()\n",
    "        self.ema2_weights, self.ema2_biases, self.ema2_activations = make_ema_params()\n",
    "        self.ema3_weights, self.ema3_biases, self.ema3_activations = make_ema_params()\n",
    "\n",
    "        self.ema_hidden_channels = ema_hidden_channels\n",
    "        self.ema_hidden_layers = ema_hidden_layers\n",
    "        self.ema_conv_kernel = ema_conv_kernel\n",
    "        self.ema_fusion_kernel = ema_fusion_kernel\n",
    "        self.ema_skip_connection = ema_skip_connection\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                block.append(ResnetBlock(\n",
    "                    in_channels=block_in,\n",
    "                    out_channels=block_out,\n",
    "                    temb_channels=self.temb_ch,\n",
    "                    dropout=dropout,)\n",
    "                )\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
    "                curr_res *= 2\n",
    "            self.up.insert(0, up)\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, 3, 1, 1)\n",
    "\n",
    "    def _conv_ema(self, h, mem_stab, mem_unstab, weights, biases, activations):\n",
    "        unsqueeze = h.ndim == 3\n",
    "        if unsqueeze:\n",
    "            h = h.unsqueeze(1)\n",
    "\n",
    "        if mem_unstab is not None:\n",
    "            q = torch.cat([h, mem_stab, mem_unstab], dim=1)\n",
    "            q = nn.functional.conv2d(q, weights[0], biases[0], padding=\"same\")\n",
    "            q = activations[0](q)\n",
    "            skip = q\n",
    "            for i in range(self.ema_hidden_layers):\n",
    "                q = nn.functional.conv2d(q, weights[i+1], biases[i+1], padding=\"same\")\n",
    "                q = activations[i+1](q)\n",
    "            if self.ema_skip_connection:\n",
    "                q = q + skip\n",
    "\n",
    "            shape = h.shape\n",
    "            head = nn.functional.conv2d(q, weights[-1], biases[-1], padding=\"same\")\n",
    "            head = rearrange(head, \"b (c p) h w -> b c p (h w)\", c=shape[1])\n",
    "            eta = torch.cat([head, torch.zeros_like(head[:, :, :1])], dim=2)\n",
    "            eta = eta.softmax(dim=2)\n",
    "            mem = nn.functional.unfold(mem_stab, self.ema_fusion_kernel, padding=self.ema_fusion_kernel // 2)\n",
    "            mem = rearrange(mem, \"b (c p) hw -> b c p hw\", c=shape[1])\n",
    "            h_flat = rearrange(h, \"b c h w -> b c (h w)\")\n",
    "            h = (mem * eta[:, :, :-1]).sum(dim=2) + eta[:, :, -1] * h_flat\n",
    "            h = h.view(shape)\n",
    "\n",
    "\n",
    "        mem_stab = h.clone()\n",
    "        mem_unstab = h.clone()\n",
    "        if self.detach_memory:\n",
    "            mem_stab = mem_stab.detach()\n",
    "            mem_unstab = mem_unstab.detach()\n",
    "\n",
    "        if unsqueeze:\n",
    "            h = h.squeeze(1)\n",
    "\n",
    "        return h, mem_stab, mem_unstab\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.conv_in(z)\n",
    "\n",
    "        # --- mid.block_1 + EMA ---\n",
    "        h = self.mid.block_1(h, None)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h, self.ema_mem_block1_stab, self.ema_mem_block1_unstab = self._conv_ema(\n",
    "            h, self.ema_mem_block1_stab, self.ema_mem_block1_unstab,\n",
    "            self.ema1_weights, self.ema1_biases, self.ema1_activations\n",
    "        )\n",
    "\n",
    "        # --- mid.block_2 + EMA ---\n",
    "        h = self.mid.block_2(h, None)\n",
    "        h, self.ema_mem_block2_stab, self.ema_mem_block2_unstab = self._conv_ema(\n",
    "            h, self.ema_mem_block2_stab, self.ema_mem_block2_unstab,\n",
    "            self.ema2_weights, self.ema2_biases, self.ema2_activations\n",
    "        )\n",
    "\n",
    "        # --- upsampling ---\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                h = self.up[i_level].block[i_block](h, None)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        # --- conv_out + EMA ---\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        h, self.ema_mem_convout_stab, self.ema_mem_convout_unstab = self._conv_ema(\n",
    "            h, self.ema_mem_convout_stab, self.ema_mem_convout_unstab,\n",
    "            self.ema3_weights, self.ema3_biases, self.ema3_activations\n",
    "        )\n",
    "\n",
    "        if self.tanh_out:\n",
    "            h = torch.tanh(h)\n",
    "\n",
    "        return h\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
