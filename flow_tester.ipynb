{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a55ffee",
   "metadata": {},
   "source": [
    "## Test All Things Flow for Stage 3 before training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c03b2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8474bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argar/miniconda3/envs/hypir/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from typing import Sequence, Dict, Union, List, Mapping, Any, Optional\n",
    "import torch.utils.data as data\n",
    "import numpy.typing as npt\n",
    "from gqvr.dataset.utils import load_video_file_list, center_crop_arr, random_crop_arr, srgb_to_linearrgb, emulate_spc\n",
    "from gqvr.utils.common import instantiate_from_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7fa0f",
   "metadata": {},
   "source": [
    "### Video Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca642275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPCVideoDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "        Dataset for finetuning the VAE's encoder and SPC-ControlNet Stages (independent of each other).\n",
    "        Args:\n",
    "            file_list (str): Path to the file list containing image paths and optionally prompts.\n",
    "            file_backend_cfg (Mapping[str, Any]): Configuration for the file backend to load images.\n",
    "            out_size (int): The output size of the images after cropping.\n",
    "            crop_type (str): Type of cropping to apply to the images. Options are 'none', 'center', or 'random'.\n",
    "        Returns:\n",
    "            A dictionary containing:\n",
    "                - 'gt': Ground truth image tensor of shape (C, H, W) with pixel values in the range [-1, 1].\n",
    "                - 'lq': SPC image (dubbed as low-quality) tensor of shape (C, H, W) with pixel values in the range [-1, 1].\n",
    "                - 'prompt': The prompt associated with the image.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                    file_list: str,\n",
    "                    file_backend_cfg: Mapping[str, Any],\n",
    "                    out_size: int,\n",
    "                    crop_type: str,\n",
    "                    use_hflip: bool) -> \"SPCVideoDataset\":\n",
    "\n",
    "        super(SPCVideoDataset, self).__init__()\n",
    "        self.file_list = file_list\n",
    "        self.video_files = load_video_file_list(file_list)\n",
    "        self.file_backend = instantiate_from_config(file_backend_cfg)\n",
    "        self.out_size = out_size\n",
    "        self.crop_type = crop_type\n",
    "        self.use_hflip = use_hflip # No need for 1.5M big dataset\n",
    "        assert self.crop_type in [\"none\", \"center\", \"random\"]\n",
    "        self.HARDDISK_DIR = \"/mnt/disks/behemoth/datasets/\"\n",
    "\n",
    "\n",
    "    def load_gt_images(self, video_path: str, max_retry: int = 5):\n",
    "        gt_images = []\n",
    "        # print(f\"Loading GT video from {video_path}\")\n",
    "        for img_name in sorted(os.listdir(video_path)):\n",
    "            image_path = os.path.join(video_path, img_name)\n",
    "            # print(f\"Loading {image_path}\")\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            # print(f\"Loaded GT image size: {image.size}\")\n",
    "            if self.crop_type != \"none\":\n",
    "                if image.height == self.out_size and image.width == self.out_size:\n",
    "                    image = np.array(image)\n",
    "                else:\n",
    "                    if self.crop_type == \"center\":\n",
    "                        image = center_crop_arr(image, self.out_size)\n",
    "                    elif self.crop_type == \"random\":\n",
    "                        image = random_crop_arr(image, self.out_size, min_crop_frac=0.7)\n",
    "            else:\n",
    "                assert image.height == self.out_size and image.width == self.out_size\n",
    "                image = np.array(image)\n",
    "            # hwc, rgb, 0,255, uint8\n",
    "            gt_images.append(image)\n",
    "        return np.stack(gt_images, axis=0) # thwc\n",
    "\n",
    "\n",
    "    def generate_spc_from_gt(self, img_gt):\n",
    "        if img_gt is None:\n",
    "            return None\n",
    "        img = srgb_to_linearrgb(img_gt / 255.)\n",
    "        img = emulate_spc(img, \n",
    "                          factor=1.0 # Brightness directly proportional to this hparam. 1.0 => scene's natural lighting\n",
    "                        )\n",
    "        return img\n",
    "\n",
    "\n",
    "    def convert_to_Nbit_spc(self, imgs_gt: npt.NDArray, bits: int = 3):\n",
    "        N = 2**bits - 1\n",
    "        imgs_lq = []\n",
    "        for img_gt in imgs_gt:\n",
    "            img_lq_sum = np.zeros_like(img_gt, dtype=np.float32)\n",
    "            for i in range(N): # 4-bit (2**4 - 1)\n",
    "                img_lq_sum = img_lq_sum + self.generate_spc_from_gt(img_gt)\n",
    "            img_lq = img_lq_sum / (1.0*N)\n",
    "            imgs_lq.append(img_lq)\n",
    "        return np.stack(imgs_lq, axis=0) # thwc\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Union[np.ndarray, str]]:\n",
    "        # load gt image\n",
    "        imgs_gt = None\n",
    "        imgs_lq = None\n",
    "        while imgs_gt is None and imgs_lq is None:\n",
    "            # load meta file\n",
    "            video_path = self.video_files[index]['video_path']\n",
    "            gt_video_path =  self.HARDDISK_DIR + video_path[2:]\n",
    "            # print(\"gt path:\", gt_path)\n",
    "            # print(f\"Loading GT image from {gt_path}\")\n",
    "            prompt = self.video_files[index]['prompt']\n",
    "\n",
    "            try:\n",
    "                imgs_gt = self.load_gt_images(gt_video_path)\n",
    "                # print(f\"Loaded {imgs_gt.shape[0]} frames from {gt_video_path}\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Could not load: {gt_video_path}, setting a random index\")\n",
    "                index = random.randint(0, len(self) - 1)\n",
    "                continue\n",
    "            \n",
    "            if imgs_gt is None:\n",
    "                print(f\"failed to load {gt_video_path} or generate lq image, try another image\")\n",
    "                index = random.randint(0, len(self) - 1)\n",
    "                continue\n",
    "\n",
    "            # NOTE: SPAD bit-resolution was changed permanently --- No need for 1-bit VAEs\n",
    "            # However, to revert back... uncomment:\n",
    "            # img_lq = self.generate_spc_from_gt(img_gt)\n",
    "            # And comment the following\n",
    "            \n",
    "            spc_bits = 3 # bits\n",
    "            imgs_lq = self.convert_to_Nbit_spc(imgs_gt, bits=spc_bits)\n",
    "\n",
    "\n",
    "        # Shape: (t, h, w, c); channel order: RGB; image range: [0, 1], float32.\n",
    "        imgs_gt = (imgs_gt / 255.0).astype(np.float32)\n",
    "        imgs_lq = imgs_lq.astype(np.float32) # BUG-FIXED now!!! for all datasets img_lq is already [0,1], no need to divide by 255\n",
    "\n",
    "        # if self.use_hflip and np.random.uniform() < 0.5:\n",
    "        #     img_gt = np.fliplr(img_gt)\n",
    "        #     img_lq = np.fliplr(img_lq)\n",
    "\n",
    "        # Should lq be normalized to [-1,1] or stay in [0, 1] range? For now [-1, 1]\n",
    "        gt = (imgs_gt * 2 - 1).astype(np.float32)\n",
    "        # [-1, 1]\n",
    "        lq = (imgs_lq * 2 - 1).astype(np.float32) \n",
    "        # print(np.amax(lq), np.amin(lq))\n",
    "        return gt, lq, prompt, gt_video_path\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.video_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84961395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Dataset length: 10\n",
      "Sample GT shape: (32, 512, 512, 3), LQ shape: (32, 512, 512, 3), Prompt: , Video Path: /mnt/disks/behemoth/datasets/UDM10_video/006\n",
      "GT Range: 1.0 | -1.0\n",
      "SPC Range: 1.0 | -1.0\n"
     ]
    }
   ],
   "source": [
    "dataset = SPCVideoDataset(\n",
    "        file_list=\"/home/argar/apgi/gQVR/dataset_txt_files/udm10_video.txt\",\n",
    "        file_backend_cfg={\"target\": \"gqvr.dataset.file_backend.HardDiskBackend\"},\n",
    "        out_size=512,\n",
    "        crop_type=\"center\",\n",
    "        use_hflip=False,\n",
    "    )\n",
    "print(f\"Complete Dataset length: {len(dataset)}\")\n",
    "sample = next(iter(dataset))\n",
    "print(f\"Sample GT shape: {sample[0].shape}, LQ shape: {sample[1].shape}, Prompt: {sample[2]}, Video Path: {sample[3]}\")\n",
    "print(f\"GT Range: {np.amax(sample[0])} | {np.amin(sample[0])}\")\n",
    "print(f\"SPC Range: {np.amax(sample[1])} | {np.amin(sample[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193cb3e",
   "metadata": {},
   "source": [
    "### Compute RAFT Flow & Viz (Using pipeline implemented code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd17807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use sdp attention as default\n",
      "keep default attention mode\n"
     ]
    }
   ],
   "source": [
    "from gqvr.model.core_raft.raft import RAFT\n",
    "from gqvr.model.core_raft.utils import flow_viz\n",
    "from gqvr.model.core_raft.utils.utils import InputPadder\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef62ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551b138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.RAFT_args object at 0x7f5418aecee0>\n"
     ]
    }
   ],
   "source": [
    "class RAFT_args:\n",
    "    mixed_precision = False\n",
    "    alternate_corr = False\n",
    "    small = False\n",
    "    dropout = False\n",
    "\n",
    "raft_args = RAFT_args()\n",
    "print(raft_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f22a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAFT(\n",
       "  (fnet): BasicEncoder(\n",
       "    (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm3): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (cnet): BasicEncoder(\n",
       "    (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (layer1): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (update_block): BasicUpdateBlock(\n",
       "    (encoder): BasicMotionEncoder(\n",
       "      (convc1): Conv2d(324, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convc2): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (convf1): Conv2d(2, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (convf2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (gru): SepConvGRU(\n",
       "      (convz1): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
       "      (convr1): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
       "      (convq1): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
       "      (convz2): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
       "      (convr2): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
       "      (convq2): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
       "    )\n",
       "    (flow_head): FlowHead(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (mask): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raft_model = RAFT(raft_args)\n",
    "# print(raft_model)\n",
    "# print(\"--------------------------------------------------------------------------\")\n",
    "raft_things_dict = torch.load(\"/home/argar/apgi/gQVR/pretrained_checkpoints/raft_models_weights/raft-things.pth\")\n",
    "corrected_state_dict = {}\n",
    "for k, v in raft_things_dict.items():\n",
    "    k2 = \".\".join(k.split(\".\")[1:])\n",
    "    corrected_state_dict[k2] = v\n",
    "raft_model.load_state_dict(corrected_state_dict)\n",
    "raft_model.eval().requires_grad_(False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60662259",
   "metadata": {},
   "source": [
    "### Test Differentiable Warping & Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cd27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable_warp(x, flow):\n",
    "    \"\"\"\n",
    "    Warp image or feature x according to flow.\n",
    "    x: [B, C, H, W]\n",
    "    flow: [B, 2, H, W] (flow in pixels, with flow[:,0] = dx, flow[:,1] = dy)\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.size()\n",
    "    # Create mesh grid normalized to [-1,1]\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W))\n",
    "    grid = torch.stack((grid_x, grid_y), 2).float().to(x.device)  # [H, W, 2]\n",
    "\n",
    "    grid = grid.unsqueeze(0).repeat(B, 1, 1, 1)  # [B, H, W, 2]\n",
    "\n",
    "    # Add flow, normalize grid to [-1,1]\n",
    "    flow = flow.permute(0, 2, 3, 1)\n",
    "    new_grid = grid + flow\n",
    "    new_grid[..., 0] = 2.0 * new_grid[..., 0] / (W - 1) - 1.0\n",
    "    new_grid[..., 1] = 2.0 * new_grid[..., 1] / (H - 1) - 1.0\n",
    "\n",
    "    warped = F.grid_sample(x, new_grid, align_corners=True)\n",
    "    return warped\n",
    "\n",
    "\n",
    "def compute_flow_magnitude(flow):\n",
    "    return torch.norm(flow, dim=1, keepdim=True)  # [B, 1, H, W]\n",
    "\n",
    "def compute_flow_gradients(flow):\n",
    "    # flow: [B, 2, H, W]\n",
    "    fx = flow[:, 0:1, :, :]  # horizontal flow\n",
    "    fy = flow[:, 1:2, :, :]  # vertical flow\n",
    "\n",
    "    # finite difference gradients (simple Sobel or central differences)\n",
    "    fx_du = fx[:, :, :, 2:] - fx[:, :, :, :-2]  # d/dx\n",
    "    fx_dv = fx[:, :, 2:, :] - fx[:, :, :-2, :]  # d/dy\n",
    "\n",
    "    fy_du = fy[:, :, :, 2:] - fy[:, :, :, :-2]\n",
    "    fy_dv = fy[:, :, 2:, :] - fy[:, :, :-2, :]\n",
    "\n",
    "    # pad to original size (pad 1 pixel on each side)\n",
    "    fx_du = F.pad(fx_du, (1, 1, 0, 0))\n",
    "    fx_dv = F.pad(fx_dv, (0, 0, 1, 1))\n",
    "    fy_du = F.pad(fy_du, (1, 1, 0, 0))\n",
    "    fy_dv = F.pad(fy_dv, (0, 0, 1, 1))\n",
    "\n",
    "    return fx_du, fx_dv, fy_du, fy_dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f3562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gqvr.model.warp_utils import flow_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "636d27c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/argar/apgi/gQVR/gqvr/model/core_raft/raft.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "/home/argar/apgi/gQVR/gqvr/model/core_raft/raft.py:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "/home/argar/miniconda3/envs/hypir/lib/python3.10/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/argar/apgi/gQVR/gqvr/model/core_raft/raft.py:123: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "gt, lq, prompt, vid_path = next(iter(dataset))\n",
    "# print(gt.shape, lq.shape, prompt, vid_path)\n",
    "frame1 = torch.Tensor(gt[0]).permute(2,0,1).unsqueeze(0).to(device)\n",
    "frame2 = torch.Tensor(gt[1]).permute(2,0,1).unsqueeze(0).to(device)\n",
    "# print(frame1.size(), frame2.size())\n",
    "\n",
    "flow_fw, flow_bw = raft_model(frame1, frame2, iters=20, test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065084ca",
   "metadata": {},
   "source": [
    "### Test Differentiable Occlusion Mask Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_occlusion(fw_flow, bw_flow, img):\n",
    "    \"\"\"\n",
    "    fw_flow: forward flow from img1 to img2, [B, 2, H, W]\n",
    "    bw_flow: backward flow from img2 to img1, [B, 2, H, W]\n",
    "    img: image tensor (for warping), [B, C, H, W]\n",
    "\n",
    "    Returns:\n",
    "        occlusion mask [B, 1, H, W], float tensor (0 or 1 mask)\n",
    "        warped_img2: img warped back to img1 space by bw_flow\n",
    "    \"\"\"\n",
    "\n",
    "    # Warp forward flow to img2 frame using backward flow\n",
    "    fw_flow_warped = differentiable_warp(fw_flow, bw_flow)  # [B, 2, H, W]\n",
    "\n",
    "    # Warp img to img1 space using backward flow\n",
    "    warp_img = differentiable_warp(img, bw_flow)\n",
    "\n",
    "    # Forward-backward flow consistency check\n",
    "    fb_flow_sum = fw_flow_warped + bw_flow  # should be near zero if consistent\n",
    "\n",
    "    fb_flow_mag = compute_flow_magnitude(fb_flow_sum)  # [B,1,H,W]\n",
    "    fw_flow_w_mag = compute_flow_magnitude(fw_flow_warped)\n",
    "    bw_flow_mag = compute_flow_magnitude(bw_flow)\n",
    "\n",
    "    threshold = 0.01 * (fw_flow_w_mag + bw_flow_mag) + 0.5\n",
    "\n",
    "    mask1 = fb_flow_mag > threshold  # bool mask [B,1,H,W]\n",
    "\n",
    "    # Compute flow gradients for motion boundary detection\n",
    "    fx_du, fx_dv, fy_du, fy_dv = compute_flow_gradients(bw_flow)\n",
    "    fx_mag = fx_du ** 2 + fx_dv ** 2\n",
    "    fy_mag = fy_du ** 2 + fy_dv ** 2\n",
    "\n",
    "    mask2 = (fx_mag + fy_mag) > 0.01 * bw_flow_mag + 0.002\n",
    "\n",
    "    # Combine masks\n",
    "    mask = mask1 | mask2  # logical or\n",
    "\n",
    "    occlusion = mask.float()  # convert to float mask (0 or 1)\n",
    "\n",
    "    return occlusion, warp_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a87e0ed",
   "metadata": {},
   "source": [
    "### Is Differentiable Warp loss ~= E*_warp loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56dafa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flow_loss(pred_frames, gt_frames):\n",
    "    B, T, C, H, W = pred_frames.shape\n",
    "    loss = 0.0\n",
    "    for i in range(T - 1):\n",
    "        frame1 = pred_frames[:, i, ...]\n",
    "        frame2 = gt_frames[:, i+1, ...]\n",
    "        # Compute forward and backward flow using RAFT\n",
    "        flow_fw, flow_bw = self.raft_model(frame1, frame2, iters=20, test_mode=True)\n",
    "        # Differentiable Warp gt frame (t+1) to pred frame (t)\n",
    "        warp_img2 = differentiable_warp(frame2, flow_fw)\n",
    "        # Differentiable Compute occlusion mask \n",
    "        occ_mask, _ = detect_occlusion(flow_fw, flow_bw)\n",
    "        noc_mask = 1 - occ_mask\n",
    "        diff = (warp_img2 - frame1) * noc_mask\n",
    "        diff_squared = diff ** 2\n",
    "        N = torch.sum(noc_mask)\n",
    "        N = torch.clamp(N, min=1.0)\n",
    "        err += torch.sum(diff_squared) / N\n",
    "    warping_error = err / (T - 1)\n",
    "    return warping_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f698584",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
